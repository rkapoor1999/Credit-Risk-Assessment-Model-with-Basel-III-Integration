{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j3/n1zgv5yd3h9_lk6htkm9rmbc0000gn/T/ipykernel_8886/1797879524.py:1: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  loans_df = pd.read_csv('../data/data.csv')\n"
     ]
    }
   ],
   "source": [
    "loans_df = pd.read_csv('../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with highest missing values:\n",
      "member_id                                     2260701\n",
      "orig_projected_additional_accrued_interest    2252050\n",
      "hardship_end_date                             2249784\n",
      "hardship_start_date                           2249784\n",
      "hardship_type                                 2249784\n",
      "hardship_reason                               2249784\n",
      "hardship_status                               2249784\n",
      "deferral_term                                 2249784\n",
      "hardship_last_payment_amount                  2249784\n",
      "hardship_payoff_balance_amount                2249784\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_counts = loans_df.isnull().sum()\n",
    "print(\"Columns with highest missing values:\")\n",
    "print(missing_counts.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature selection - Drop columns with too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 44 columns with more than 50% missing values\n"
     ]
    }
   ],
   "source": [
    "missing_threshold = 0.5  # Drop columns with > 50% missing values\n",
    "cols_to_drop = [col for col in loans_df.columns if loans_df[col].isnull().sum() / len(loans_df) > missing_threshold]\n",
    "print(f\"Dropping {len(cols_to_drop)} columns with more than 50% missing values\")\n",
    "loans_df = loans_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle missing values for key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'emp_length' in loans_df.columns and loans_df['emp_length'].isnull().sum() > 0:\n",
    "    loans_df['emp_length'].fillna(loans_df['emp_length'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTI - replace with median based on grade\n",
    "if 'dti' in loans_df.columns and loans_df['dti'].isnull().sum() > 0:\n",
    "    if 'grade' in loans_df.columns:\n",
    "        # Group by grade and fill with median DTI for that grade\n",
    "        loans_df['dti'] = loans_df.groupby('grade')['dti'].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "    else:\n",
    "        # Just use overall median\n",
    "        loans_df['dti'].fillna(loans_df['dti'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit history features - often missing means \"no events\"\n",
    "zero_fill_columns = ['delinq_2yrs', 'inq_last_6mths', 'pub_rec', \n",
    "                    'revol_util', 'collections_12_mths_ex_med']\n",
    "for col in zero_fill_columns:\n",
    "    if col in loans_df.columns and loans_df[col].isnull().sum() > 0:\n",
    "        loans_df[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining missing values in selected features:\n"
     ]
    }
   ],
   "source": [
    "# Check if any missing values remain in key modeling features\n",
    "print(\"\\nRemaining missing values in selected features:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable (loan status)\n",
    "# Convert to binary: 1 for default, 0 for fully paid\n",
    "loans_df['loan_status_binary'] = loans_df['loan_status'].apply(\n",
    "    lambda x: 1 if x in ['Charged Off', 'Default', 'Late (31-120 days)'] else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['loan_amnt', 'term', 'int_rate', 'installment', 'grade',\n",
    "          'emp_length', 'home_ownership', 'annual_inc', 'dti',\n",
    "          'delinq_2yrs', 'fico_range_low', 'fico_range_high', 'inq_last_6mths',\n",
    "          'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt          33\n",
      "term               33\n",
      "int_rate           33\n",
      "installment        33\n",
      "grade              33\n",
      "emp_length          0\n",
      "home_ownership     33\n",
      "annual_inc         37\n",
      "dti                33\n",
      "delinq_2yrs         0\n",
      "fico_range_low     33\n",
      "fico_range_high    33\n",
      "inq_last_6mths      0\n",
      "open_acc           62\n",
      "pub_rec             0\n",
      "revol_bal          33\n",
      "revol_util          0\n",
      "total_acc          62\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(loans_df[features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in selected features\n",
    "loans_df = loans_df.dropna(subset=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After dropping rows, missing values in selected features:\n",
      "loan_amnt          0\n",
      "term               0\n",
      "int_rate           0\n",
      "installment        0\n",
      "grade              0\n",
      "emp_length         0\n",
      "home_ownership     0\n",
      "annual_inc         0\n",
      "dti                0\n",
      "delinq_2yrs        0\n",
      "fico_range_low     0\n",
      "fico_range_high    0\n",
      "inq_last_6mths     0\n",
      "open_acc           0\n",
      "pub_rec            0\n",
      "revol_bal          0\n",
      "revol_util         0\n",
      "total_acc          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify no missing values remain\n",
    "print(\"\\nAfter dropping rows, missing values in selected features:\")\n",
    "print(loans_df[features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining rows: 2260639 (dropped 62 rows)\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows were removed\n",
    "print(f\"\\nRemaining rows: {loans_df.shape[0]} (dropped {2260701 - loans_df.shape[0]} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processoing on batches to avoid kernel crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1/23\n",
      "Processed chunk 2/23\n",
      "Processed chunk 3/23\n",
      "Processed chunk 4/23\n",
      "Processed chunk 5/23\n",
      "Processed chunk 6/23\n",
      "Processed chunk 7/23\n",
      "Processed chunk 8/23\n",
      "Processed chunk 9/23\n",
      "Processed chunk 10/23\n",
      "Processed chunk 11/23\n",
      "Processed chunk 12/23\n",
      "Processed chunk 13/23\n",
      "Processed chunk 14/23\n",
      "Processed chunk 15/23\n",
      "Processed chunk 16/23\n",
      "Processed chunk 17/23\n",
      "Processed chunk 18/23\n",
      "Processed chunk 19/23\n",
      "Processed chunk 20/23\n",
      "Processed chunk 21/23\n",
      "Processed chunk 22/23\n",
      "Processed chunk 23/23\n",
      "\n",
      "Missing values in encoded dataframe:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# First, identify unique values for each categorical feature\n",
    "# This ensures consistent encoding across chunks\n",
    "categorical_features = ['term', 'grade', 'emp_length', 'home_ownership']\n",
    "categorical_values = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    categorical_values[feature] = loans_df[feature].unique().tolist()\n",
    "\n",
    "# Process in smaller chunks\n",
    "chunk_size = 100000\n",
    "num_chunks = (loans_df.shape[0] // chunk_size) + 1\n",
    "encoded_chunks = []\n",
    "\n",
    "# First, create an empty DataFrame with the expected columns\n",
    "# Get the column structure from a single row\n",
    "sample = loans_df.iloc[0:1][features]\n",
    "sample_encoded = pd.get_dummies(sample, drop_first=True)\n",
    "expected_columns = sample_encoded.columns.tolist()\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, loans_df.shape[0])\n",
    "    \n",
    "    # Get the chunk with selected features\n",
    "    chunk = loans_df.iloc[start_idx:end_idx][features]\n",
    "    \n",
    "    # Get dummies with the same structure for each chunk\n",
    "    chunk_encoded = pd.get_dummies(chunk, drop_first=True)\n",
    "    \n",
    "    # Make sure all expected columns exist (this handles potential missing categories in some chunks)\n",
    "    for col in expected_columns:\n",
    "        if col not in chunk_encoded.columns:\n",
    "            chunk_encoded[col] = 0\n",
    "    \n",
    "    # Keep only the expected columns and in the same order\n",
    "    chunk_encoded = chunk_encoded[expected_columns]\n",
    "    encoded_chunks.append(chunk_encoded)\n",
    "    \n",
    "    print(f\"Processed chunk {i+1}/{num_chunks}\")\n",
    "\n",
    "# Combine all chunks\n",
    "loans_df_encoded = pd.concat(encoded_chunks)\n",
    "\n",
    "# Verify no NaNs\n",
    "print(\"\\nMissing values in encoded dataframe:\")\n",
    "print(loans_df_encoded.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti',\n",
       "       'delinq_2yrs', 'fico_range_low', 'fico_range_high', 'inq_last_6mths',\n",
       "       'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X = loans_df_encoded\n",
    "y = loans_df['loan_status_binary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
